name: Hugging Face
entries:
  - title: IDEFICS Playground
    label: Idefics
    short_description: This demo showcases IDEFICS, a open-access large visual language model.
    long_description: >-
      IDEFICS, short for Image-aware Decoder Enhanced Ã  la Flamingo with Interleaved Cross-attentionS, is an 80-billion parameter open-access visual language model. It can process both image and text inputs, enabling capabilities like image-based question answering, visual content description, and storytelling with multiple images. IDEFICS is a reproduction of Deepmind's closed-source Flamingo model, built using publicly available data. The model variants in this demo have been fine-tuned for conversational settings.      
    tags:
      - Gradio
      - Space
      - Idefics
    git_url: "https://huggingface.co/spaces/HuggingFaceM4/idefics_playground"
    is_prototype: true
    is_huggingface_space: true
    enabled: false
    is_new: true
    environment_variables: 
      cpu:
        default: 2
        description: "Number of CPUs"
      memory:
        default: 16
        description: "Memory in GB"
      gpu:
        default: 0
        description: "Number of GPUs"
      HF_AUTH_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
  - title: Code Llama Playground 13B
    label: Llama-13B
    short_description: This is a demo to generate text and code with the following Code Llama model (13B).
    long_description: >-
      Code Llama Playground showcases the 13-billion parameter Code Llama model, focused on text and code generation. It's primarily designed for code completion rather than instructional purposes or interactive chatting. Additional insights into the model's capabilities and design can be found in the associated blog post and paper.
    image_path: >-
      https://raw.githubusercontent.com/cloudera/HuggingFace-Spaces/master/images/codellama-playground.png
    tags:
      - Gradio
      - Space
      - Llama
      - 13B
    git_url: "https://huggingface.co/spaces/codellama/codellama-playground"
    is_prototype: true
    is_huggingface_space: true
    is_new: true
    environment_variables: 
      cpu:
        default: 2
        description: "Number of CPUs"
      memory:
        default: 16
        description: "Memory in GB"
      gpu:
        default: 0
        description: "Number of GPUs"
      HF_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
  - title: Open LLM Leaderboard
    label: open-LLM
    short_description: The Open LLM Leaderboard aims to track, rank and evaluate open LLMs and chatbots.
    long_description: >-
      The ðŸ¤— Open LLM Leaderboard is a platform dedicated to tracking, ranking, and evaluating open Large Language Models (LLMs) and chatbots. It offers a system for automated model evaluation on the ðŸ¤— GPU cluster. The leaderboard uses Eleuther AI's Language Model Evaluation Harness, detailed in the "About" page.
    image_path: >-
      https://raw.githubusercontent.com/cloudera/HuggingFace-Spaces/master/images/llm-leaderboard.png
    tags:
      - Gradio
      - Space
      - LLM
    git_url: "https://huggingface.co/spaces/smothiki/open_llm_leaderboard"
    is_prototype: true
    is_huggingface_space: true
    is_new: true
    environment_variables: 
      cpu:
        default: 2
        description: "Number of CPUs"
      memory:
        default: 16
        description: "Memory in GB"
      gpu:
        default: 0
        description: "Number of GPUs"
      HF_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
  - title: Mistral 7B Instruct
    label: Mistral-7B
    short_description: In this demo, you can chat with Mistral-7B-Instruct model.
    long_description: >-
      Mistral 7B Instruct showcases Mistral-7B-v0.1, Mistral AI's first Large Language Model (LLM), featuring a decoder-based architecture. Key features include Sliding Window Attention with an 8k context length, Grouped Query Attention (GQA) for faster inference, and a byte-fallback BPE tokenizer. The Mistral-7B-Instruct-v0.1 variant is instruction fine-tuned for chat-based interactions. Both models are available under the Apache 2.0 license. Further information is in the release blog post.
    image_path: >-
      https://raw.githubusercontent.com/cloudera/HuggingFace-Spaces/master/images/mistral-7B.png
    tags:
      - Gradio
      - Space
      - Llama
      - 13B
    git_url: "https://huggingface.co/spaces/osanseviero/mistral-super-fast"
    is_prototype: true
    is_huggingface_space: true
    is_new: true
    environment_variables: 
      cpu:
        default: 2
        description: "Number of CPUs"
      memory:
        default: 16
        description: "Memory in GB"
      gpu:
        default: 0
        description: "Number of GPUs"
      HF_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
      HUGGING_FACE_HUB_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
  - title: Chat with DeepSeek Coder 33B
    label: DeepSeek-33B
    short_description: In this demo, you can make use of DeepSeek 33B parameters model.
    long_description: >-
      DeepSeek-33B-Chat features the DeepSeek-Coder model, a 33-billion parameter code model fine-tuned for chat instructions. This space is dedicated to demonstrating its capabilities in handling chat-based coding queries.
    image_path: >-
      https://raw.githubusercontent.com/cloudera/HuggingFace-Spaces/master/images/deepseek-33b-chat.png
    tags:
      - Gradio
      - Space
      - DeepSeek
      - 33B
    git_url: "https://huggingface.co/spaces/deepseek-ai/deepseek-coder-33b-instruct"
    is_prototype: true
    is_huggingface_space: true
    is_new: true
    environment_variables: 
      cpu:
        default: 4
        description: "Number of CPUs"
      memory:
        default: 32
        description: "Memory in GB"
      gpu:
        default: 4
        description: "Number of GPUs"
      HF_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
      HUGGING_FACE_HUB_TOKEN:
        default: ""
        description: "Hugging Face Token (HuggingFace->Settings->Access Tokens)"
        required: true
        tooltip: "Get your token from HuggingFace->Settings->Access Tokens"
  - title: Can You Run It? LLM version 
    label: Run-llm-version
    short_description: This is a demo to check if LLM can be executed,
    long_description: >-
      Can you run it? LLM version" is a tool designed to help users assess the feasibility of training or running large language models (LLMs). It provides guidance on the necessary GPU resources and quantization levels for different LLMs.
    image_path: >-
      https://raw.githubusercontent.com/cloudera/HuggingFace-Spaces/master/images/run-llm-version.png
    tags:
      - Streamlit
      - Space
      - LLM
    git_url: "https://huggingface.co/spaces/Vokturz/can-it-run-llm"
    is_prototype: true
    is_huggingface_space: true
    is_new: true
